<!-- Output copied to clipboard! -->

<!-----

Yay, no errors, warnings, or alerts!

Conversion time: 1.662 seconds.


Using this Markdown file:

1. Paste this output into your source file.
2. See the notes and action items below regarding this conversion run.
3. Check the rendered output (headings, lists, code blocks, tables) for proper
   formatting and use a linkchecker before you publish this page.

Conversion notes:

* Docs to Markdown version 1.0β33
* Thu May 19 2022 22:21:45 GMT-0700 (PDT)
* Source doc: Collation Tutorial

WARNING:
You have 12 H1 headings. You may want to use the "H1 -> H2" option to demote all headings by one level.

----->


<p style="color: red; font-weight: bold">>>>>>  gd2md-html alert:  ERRORs: 0; WARNINGs: 1; ALERTS: 0.</p>
<ul style="color: red; font-weight: bold"><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with >>>>>  gd2md-html alert:  for specific instances that need correction.</ul>

<p style="color: red; font-weight: bold">Links to alert messages:</p>
<p style="color: red; font-weight: bold">>>>>> PLEASE check and correct alert issues and delete this message and the inline alerts.<hr></p>



# **Collation tutorial**


## **18 March 2022**


# Intended Audience


## Internal

This document is intended for internal users of aMDP Platform to create configuration yaml files for data Collation.   


# Introduction

Collation is the process in which data ingested from multiple sources is brought together to generate structured data that provides useful information. The collation process involves mapping the relationship between the various elements from the sources being brought together. A single record per user-defined collation key is generated for all the inputs getting collated. \
 \
Collated data structure may be useful to:  \
-build customer segments based on their exposure to advertising in multiple channels (engagement with websites) \
- build reports that illustrate customer journeys as they are exposed to or interact with advertising. \
-build reports that attribute conversion or engagement activities to exposure to advertising


# Technical Prerequisites

To be able to understand the collation use-cases and create collation configuration people are required to have the following knowledge.



* Yaml 
* Avro
* Csv
* Aws S3
* Parquet


# Access Requirements



* Access to the Prod c900 aws environment.
* Access to the documentation portal. 


# Getting Started 

In this section we will learn how to create a basic collation job that takes sample input files from s3 and generates a collated output file based on the collation key provided. 


# Creating your first collation job: Homogenous Collation

This is the simplest of all the Collation use-cases in which there is only one input. It is used for grouping the input by the keys provided for collation. The remaining fields of the input are available as a nested array of structs.  



1. Download the sandbox cli application version 3.1 from the [docs portal](https://docs-staging.aqfer.net/docs/amdp/aqsandbox/downloads/3.1.0.html).
2. Request Refresh token from Ops team to be able to run a job using sandbox for cid c008. 
3. Configure the sandbox application as described [here](https://docs.aqfer.net/docs/amdp/aqsandbox.html). Sample configuration aqsandbox.yaml for 3.1 version:

    ```
    aqfer_refresh_token: <REFRESH_TOKEN>
    cid: 'c900'
    cluster_name: aqfer-prod-eks
    ```


4. For our first job, we will  use the following input schema. The input file used for testing is available [here](https://drive.google.com/drive/folders/1o_bEyRUgJTok15-fTEp5Pvg7cWlMvS3H).

    ```
    {
       "type" : "record",
       "namespace" : "tutorial",
       "name" : "person",
       "fields" : [
          { "name" : "id" , "type" : "string" },
          { "name" : "name" , "type" : "string" },
          { "name" : "age" , "type" : "int" },
          { "name" : "is_employed", "type": "boolean" }
       ]
    }
    ```


5. The input files used for collation are available in aws S3 location - s3://com.aqfer.c900.sandbox/inputs/homogenous/in the prod c900 aws account.. 
6. To access the aws account login to aws and “switch role” to ​​DevelopersAccessRole for the c900 aws account. 
7. Create a yaml file with the following content

    ```
    work_area: s3://c900.default@com.aqfer.c900.sandbox/workarea/homogenous/
    inputs:
      - name: person
        locations:
          - folder: s3://c900.default@com.aqfer.c900.sandbox/inputs/person/
        key:
          - id
    narrow_tables: true
    sort:
      key_names:
        - id
      project:
        '.':
          field_names: [id]
    outputs:
      - destination: s3://c900.default@com.aqfer.c900.sandbox/outputs/homogenous/
        project:
          '.':
            field_names: [id, person]
    ```


8. Copy the yaml file to the config subdirectory in the sandbox cli installation folder.  
9. Delete all the content in the s3 location given in the work_area section of the configuration,  This is the location where all the intermediate files get generated during a job run.  
10. To start the job execute the following command
        1. ./aqsandbox -V collator run config/&lt;confg_file>.yaml
11. You would get an output similar to the following
        2. Launching job... done
        3. execution_id: "20220321092350"
        4. status: running
12. You can query the job for status and logs using the following
13.  		./aqsandbox -V collator status 20220321092350
        5. ./aqsandbox -V collator logs 20220321092350
14. If the job runs successfully, you will see the output file generated in the s3 location as defined in the output section in the job configuration yaml file. 
15. The schema of the output file will be

    ```
    {
      "type" : "record",
      "name" : "composite",
      "fields" : [ {
        "name" : "id",
        "type" : "string"
      }, {
        "name" : "person",
        "type" : {
          "type" : "array",
          "items" : {
            "type" : "record",
            "name" : "person",
            "fields" : [ {
              "name" : "name",
              "type" : "string"
            }, {
              "name" : "age",
              "type" : "int"
            }, {
              "name" : "is_employed",
              "type" : "boolean"
            } ]
          }
        }
      } ]
    }
    ```



    The output file for this job run is available [here](https://drive.google.com/drive/folders/1o_bEyRUgJTok15-fTEp5Pvg7cWlMvS3H). 

16. Congratulations! You have run your first collation job. 

*****Note**:

It takes about 5- 10 minutes for the job to run in the sandbox environment. Until the job starts to run you will not be able to get useful information in the logs. You may see a response similar to the example below - this error state will resolve itself once the job starts to run. 


```
rashmiramalingam@Rashmis-MacBook-Pro aqsandbox-darwin-amd64-3.1.0 % ./aqsandbox -V collator logs 20220322102211
Getting job logs... done
Logs: ""
rashmiramalingam@Rashmis-MacBook-Pro aqsandbox-darwin-amd64-3.1.0 % ./aqsandbox -V collator logs 20220322102211
Getting job logs... {"error":"application error: error getting logs for sandbox-collator-20220322102211: the server rejected our request for an unknown reason (get pods sandbox-collator-20220322102211-zmwsk)"}
```



# Next Steps

Now that we have executed a basic collation job it is time to move on to try the different collation use cases. In the forthcoming sections we will explore typical use-cases that are used across the collation jobs in production today. 


# Heterogenous Collation

Heterogenous collation is used to collate multiple input files of different schemas. 

In the sample configuration provided we will collate input files that follow the schemas given below:


##### Person Schema


```
{
   "type" : "record",
   "namespace" : "tutorial",
   "name" : "person",
   "fields" : [
      { "name" : "id" , "type" : "string" },
      { "name" : "name" , "type" : "string" },
      { "name" : "age" , "type" : "int" },
      { "name" : "is_employed", "type": "boolean" }
   ]
}
```


Person Info Schema


```
{
   "type" : "record",
   "namespace" : "tutorial",
   "name" : "person_info",
   "fields" : [
      { "name" : "id" , "type" : "string" },
      { "name" : "name" , "type" : "string" },
      { 
          "name" : "info" , 
          "type": {
             "type": "map",
             "values" : "string"
          }
      },
      { "name" : "phones" , 
        "type" : {
             "type": "array",
             "items" : 
               {
                  "type": "record",
                  "name": "phones",
                  "fields": [
                     { "name" : "landline" , "type" : "long" },
                     { "name" : "cell" , "type" : "long" }
                  ]
               }
          }
      }
   ]
}
```



##### Sample Job Configuration


```
work_area: s3://c900.default@com.aqfer.c900.sandbox/workarea/heterogenous/
inputs:
  - name: person
    locations:
      - folder: s3://c900.default@com.aqfer.c900.sandbox/inputs/person/
    key:
      - id
  - name: person_info   
    locations:
      - folder: s3://c900.default@com.aqfer.c900.sandbox/inputs/person_info/
    key:
      - id
narrow_tables: true
sort:
  key_names:
    - id
  project:
    '.':
      field_names: [id]
outputs:
  - destination: s3://c900.default@com.aqfer.c900.sandbox/outputs/heterogenous/
    project:
      '.':
        field_names: [id, person, person_info]
```



##### Output Schema


```
{
  "type" : "record",
  "name" : "composite",
  "fields" : [ {
    "name" : "id",
    "type" : "string"
  }, {
    "name" : "person",
    "type" : {
      "type" : "array",
      "items" : {
        "type" : "record",
        "name" : "person",
        "fields" : [ {
          "name" : "name",
          "type" : "string"
        }, {
          "name" : "age",
          "type" : "int"
        }, {
          "name" : "is_employed",
          "type" : "boolean"
        } ]
      }
    }
  }, {
    "name" : "person_info",
    "type" : {
      "type" : "array",
      "items" : {
        "type" : "record",
        "name" : "person_info",
        "fields" : [ {
          "name" : "name",
          "type" : "string"
        }, {
          "name" : "info",
          "type" : {
            "type" : "map",
            "values" : "string"
          }
        }, {
          "name" : "phones",
          "type" : {
            "type" : "array",
            "items" : {
              "type" : "record",
              "name" : "phones",
              "fields" : [ {
                "name" : "landline",
                "type" : "long"
              }, {
                "name" : "cell",
                "type" : "long"
              } ]
            }
          }
        } ]
      }
    }
  } ]
}
```



# Collation with Transform 

In this use case we will explore the different options available for transforming the output generated in collation.  \
 \
The available options for transformation include:  \
[Rename](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#rename) \
[ Inner](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#inner) \
[Omit](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#omit) \
[unnest](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#unnest)  \
[Python](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#python) \
[Group_by](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#groupby) \
[ Squash](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#squash) \
[ Un_union](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#ununion) \
[Sort_by](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#sortby) \
[Unwrap](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#unwrap) \
[filter](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#filter)  \



##### Sample Job Configuration Https://docs-staging.aquifer.net/docs/amdp/services/collation-service/collator/collator-schema.html


```
work_area: s3://c900.default@com.aqfer.c900.sandbox/workarea/transform/
inputs:
  - name: person
    locations:
      - folder: s3://c900.default@com.aqfer.c900.sandbox/inputs/person/
    key:
      - id
      - name
  - name: person_info
    locations:
      - folder: s3://c900.default@com.aqfer.c900.sandbox/inputs/person_info/
    key:
      - id
      - name
narrow_tables: true
split:
  bucket_size_bytes: 268435456
sort:
  key_names:
    - id
    - name
  num_writers_per_worker: 200
  project:
    '.':
      field_names: [id]
outputs:
  - destination: s3://c900.default@com.aqfer.c900.sandbox/outputs/transform/
    project:
      '.':
        field_names: [id, name, person, person_info]
transform:
  - squash:
      field_name: person
  - unwrap:
      field_name: person
  - squash:
      field_name: person_info
  - unwrap:
     field_name: person_info
```



##### Output schema after Transformation:


```
{
  "type" : "record",
  "name" : "composite",
  "fields" : [ {
    "name" : "id",
    "type" : "string"
  }, {
    "name" : "name",
    "type" : "string"
  }, {
    "name" : "age",
    "type" : "int"
  }, {
    "name" : "is_employed",
    "type" : "boolean"
  }, {
    "name" : "info",
    "type" : {
      "type" : "map",
      "values" : "string"
    }
  }, {
    "name" : "phones",
    "type" : {
      "type" : "array",
      "items" : {
        "type" : "record",
        "name" : "phones",
        "fields" : [ {
          "name" : "landline",
          "type" : "long"
        }, {
          "name" : "cell",
          "type" : "long"
        } ]
      }
    }
  } ]
}
```



# Transform using Python

This use case describes how we can use Python code snippets to transform the collated data. You can add or drop fields from the schema and can also programmatically transform each record’s data  from the Python plugin. 


##### Sample job Configuration.


```
    work_area: s3://c900.default@com.aqfer.c900.sandbox/workarea/transform/
    inputs:
      - name: person
        locations:
          - folder: s3://c900.default@com.aqfer.c900.sandbox/person/
        key:
          - id
          - name
      - name: person_info
        locations:
          - folder: s3://c900.default@com.aqfer.c900.sandbox/inputs/person_info/
        key:
          - id
          - name
    narrow_tables: true
    split:
      bucket_size_bytes: 268435456
    sort:
      key_names:
        - id
        - name
      num_writers_per_worker: 200
      project:
        '.':
          field_names: [id]
    outputs:
      - destination: s3://c900.default@com.aqfer.c900.sandbox/outputs/transform/
        project:
          '.':
            field_names: [id, name, person, person_info]
    transform:
      - python:
          code: |
              import schema    
              def dat_hnd(rec: any) -> any:
                rec["test"] = "new field"
                rec["name"] = rec["name"].upper()
                return [rec]


              def schema_handler_v1(sch: schema.Schema) -> (schema.Schema, any):
                schema.add_fields(sch, schema.Field(name="test", schema=schema.StringSchema()))
                return sch, dat_hnd
```



# Partitioning

The collated data can be partitioned using partition keys. 

The _partition_keys_ field in the config indicates the fields on which the input is partitioned and the fields by which the output needs to be partitioned. 

_capture_partition_keys_ in the Input Specification makes the input partitioning keys available as fields. 

Output partitioning keys can be specified in the [Sort Config](https://docs.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html#sortconfig) in the fields:



* _key_names_ - Names of keys for sorting
* _num_partition_keys_ - Number of keys in the key list that are partition-keys. Value of “n” for this field ensures that the first value of “n” keys in the key_names are taken as partitioning keys.  

The key_names for partitioning that are used in the SortConfig can be columns in the collated data that are already available, or can be new fields that are dynamically created using Python transformations. 


##### Sample Partitioning Configuration:


```
---
work_area: s3://c900.default@com.aqfer.c900.sandbox/tmp/
partition_keys:
  - event_date
inputs:
  - name: transaction
    locations:
      - folder: s3://c900.default@com.aqfer.c900.sandbox/inputs/partitioning/event_date=*/event_hour=*
    key:  
      - id
      - name
narrow_tables: true
sort:
  num_partition_keys: 0
  key_names:
    - id
    - name
  project:
    '.':
      field_names: [id]
outputs:
  - destination: s3://c900.default@com.aqfer.c900.sandbox/outputs/
    project:
      '.':
        field_names: [id, name, transaction]
```



# 
                            `										References

[https://docs-staging.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html](https://docs-staging.aqfer.net/docs/amdp/services/collation-service/collator/collator-schema.html)

[https://avro.apache.org/docs/current/index.html](https://avro.apache.org/docs/current/index.html)

[https://aqferinc.atlassian.net/browse/AQ-6855](https://aqferinc.atlassian.net/browse/AQ-6855)

.
